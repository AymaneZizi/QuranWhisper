{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper \n",
    "import pandas as pd \n",
    "import torch \n",
    "from transformers import pipeline \n",
    "from huggingface_hub import model_info \n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby \n",
    "import pyarabic.number as number\n",
    "import re\n",
    "from pathlib import Path\n",
    "import locale \n",
    "locale.getpreferredencoding = lambda: 'UTF-8' \n",
    "import jiwer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files(text):\n",
    "    result = []\n",
    "    regex = re.compile(\"\\[.*?\\]\")\n",
    "    temp_result = re.findall(regex, text)\n",
    "    en_letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'\n",
    "             ,'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Z','Y','Z']\n",
    "    for string in temp_result:\n",
    "        contains = any((c in string) for c in en_letters)\n",
    "        if contains:\n",
    "            result.append(string)\n",
    "    for el in result:\n",
    "        text = text.replace(el,' ')\n",
    "    return text\n",
    "def clean_some_chars(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!'\n",
    "              ,\"[\",\"]\",\"{\",\"}\",\"*\",\":\",\"#\",\"$\",\"€\",\"£\",\"~\",\"<\",\">\",\"/\",\"|\",\"'\",\",\",'=','(',')','+','•',';','&','–','♦','%'\n",
    "             ,'»','»','·']\n",
    "    \n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ',' ',' '\n",
    "               ,\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",' ',' ',' ',' ',' ',' ',' ',' ',' ',' '\n",
    "              ,' ',' ',' ']\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    text = text.replace('\\'','')\n",
    "    # removing numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "def clean_english_chars(text):\n",
    "    search = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'\n",
    "             ,'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Z','Y','Z']\n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i],' ')\n",
    "    return text\n",
    "def remove_unnecessary_spaces(text):\n",
    "    return re.sub(' +',' ',text)\n",
    "def remove_non_arabic_letters(text):\n",
    "    \n",
    "    '''\n",
    "    ALEF_MADDA       = u'\\u0622' \n",
    "    ALEF_HAMZA_ABOVE = u'\\u0623' \n",
    "    WAW_HAMZA        = u'\\u0624' \n",
    "    ALEF_HAMZA_BELOW = u'\\u0625' \n",
    "    YEH_HAMZA        = u'\\u0626' \n",
    "    ALEF             = u'\\u0627' \n",
    "    BEH              = u'\\u0628' \n",
    "    TEH_MARBUTA      = u'\\u0629' \n",
    "    TEH              = u'\\u062a' \n",
    "    THEH             = u'\\u062b' \n",
    "    JEEM             = u'\\u062c' \n",
    "    HAH              = u'\\u062d' \n",
    "    KHAH             = u'\\u062e' \n",
    "    DAL              = u'\\u062f' \n",
    "    THAL             = u'\\u0630' \n",
    "    REH              = u'\\u0631' \n",
    "    ZAIN             = u'\\u0632' \n",
    "    SEEN             = u'\\u0633' \n",
    "    SHEEN            = u'\\u0634' \n",
    "    SAD              = u'\\u0635' \n",
    "    DAD              = u'\\u0636' \n",
    "    TAH              = u'\\u0637' \n",
    "    ZAH              = u'\\u0638' \n",
    "    AIN              = u'\\u0639' \n",
    "    GHAIN            = u'\\u063a' \n",
    "    TATWEEL          = u'\\u0640' \n",
    "    FEH              = u'\\u0641' \n",
    "    QAF              = u'\\u0642' \n",
    "    KAF              = u'\\u0643' \n",
    "    LAM              = u'\\u0644' \n",
    "    MEEM             = u'\\u0645' \n",
    "    NOON             = u'\\u0646' \n",
    "    HEH              = u'\\u0647' \n",
    "    WAW              = u'\\u0648' \n",
    "    ALEF_MAKSURA     = u'\\u0649' \n",
    "    YEH              = u'\\u064a' \n",
    "    MADDA_ABOVE      = u'\\u0653' \n",
    "    HAMZA_ABOVE      = u'\\u0654' \n",
    "    HAMZA_BELOW      = u'\\u0655' \n",
    "    LAM_ALEF                     = u'\\ufefb' \n",
    "    LAM_ALEF_HAMZA_ABOVE         = u'\\ufef7' \n",
    "    LAM_ALEF_HAMZA_BELOW         = u'\\ufef9' \n",
    "    LAM_ALEF_MADDA_ABOVE         = u'\\ufef5' \n",
    "    '''\n",
    "    \n",
    "    regex = re.compile(r'[\\u0622\\u0623\\u0624\\u0625\\u0626\\u0627\\u0628\\u0629\\u062a\\u062b\\u062c\\u062d\\u062e\\u062f\\u0630\\u0631\\u0632\\u0633\\u0634\\u0635\\u0636\\u0637\\u0638\\u0639\\u063a\\u0640\\u0641\\u0642\\u0643\\u0644\\u0645\\u0646\\u0647\\u0648\\u0649\\u064a\\u0653\\u0654\\u0655\\ufefb\\ufef7\\ufef9\\ufef5]')\n",
    "    # removing Arabic letters from the text and storing the result in the varialbe: unwanted_str .\n",
    "    unwanted_str = regex.sub(' ',text)\n",
    "    # Creating a list containing all of the unwanted characters, letters and symbols.\n",
    "    unwanted_list_of_strs = list(unwanted_str.replace(\" \", \"\"))\n",
    "    # Cleaning the unwanted list of characters out of the text\n",
    "    for i in range(0, len(unwanted_list_of_strs)):\n",
    "        text = text.replace(unwanted_list_of_strs[i], \" \")\n",
    "    \n",
    "    text = remove_unnecessary_spaces(text)\n",
    "    \n",
    "    return text\n",
    "def concatenate_list_into_string(lis_strs):\n",
    "    result = \"\"\n",
    "    for el in lis_strs:\n",
    "        result += \" \" + el\n",
    "    return result\n",
    "def remove_single_letters(text):\n",
    "    words = text.split(' ')\n",
    "    waw = 'و'\n",
    "    for word in words:\n",
    "        if len(word.strip()) == 1:\n",
    "            if word != waw:\n",
    "                words.remove(word)\n",
    "    text = concatenate_list_into_string(words)\n",
    "    return text\n",
    "def clean_text(text):\n",
    "    # removing files\n",
    "    text = remove_files(text)\n",
    "    # removing some unuseful chars\n",
    "    text = clean_some_chars(text)\n",
    "    # removing english chars\n",
    "    text = clean_english_chars(text)\n",
    "    # removing tashkeel\n",
    "    text = araby.strip_tashkeel(text)\n",
    "    # removing longation\n",
    "    text = araby.strip_tatweel(text)\n",
    "    # removing unwanted spaces\n",
    "    text = remove_unnecessary_spaces(text)\n",
    "    # removing non-arabic characters\n",
    "    text = remove_non_arabic_letters(text)\n",
    "    # removing single unwanted letters\n",
    "    text = remove_single_letters(text)\n",
    "    # returning result\n",
    "    return text    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Whisper and WhisperAR models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quran Reference, retrieved from tanzil.net \n",
    "df_ref = pd.read_csv(\"quran-simple-clean.txt\",header=None) \n",
    "vals_groundtruth = df_ref[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI's Whisper - Large ~ requires 10GB of VRAM, otherwise it will through a CUDA out of memory error\n",
    "model = whisper.load_model(\"large\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transcriptions_list = [] \n",
    "file_list = [] \n",
    "time_list = [] \n",
    "directory = '/Path to .wav audio files' \n",
    "files = sorted(Path(directory).glob('*.wav')) \n",
    "for i, file in tqdm(enumerate(files)): \n",
    "    start = time.time() \n",
    "    Transcriptions_list.append(model.transcribe(str(file),language='ar')['text']) \n",
    "    end = time.time() \n",
    "    file_list.append(str(file).split('/')[-1]) \n",
    "    time_list.append(end-start) \n",
    "df = pd.DataFrame({'File_no':file_list,\n",
    "                   'Transcription':Transcriptions_list,\n",
    "                   'Execution_time':time_list})\n",
    "df['Transcription'] = df['Transcription'].apply(clean_text) \n",
    "vals = df['Transcription'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate against Quran reference \n",
    "wer_list = [] \n",
    "mer_list = [] \n",
    "cer_list = [] \n",
    "wil_list = [] \n",
    "for i, j in zip(vals,vals_groundtruth): \n",
    "  wer_list.append(jiwer.wer(j,i)) \n",
    "  mer_list.append(jiwer.mer(j,i)) \n",
    "  cer_list.append(jiwer.cer(j,i)) \n",
    "  wil_list.append(jiwer.wil(j,i))\n",
    "df['WER'] = wer_list \n",
    "df['MER'] = mer_list \n",
    "df['CER'] = cer_list \n",
    "df['WIL'] = wil_list \n",
    "df.to_csv(\"whisper_eval.csv\",index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WhisperAR - Large \n",
    "MODEL_NAME = 'arbml/whisper-largev2-ar'\n",
    "lang = 'ar'\n",
    "device = 0 if torch.cuda.is_available() else 'cpu' \n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=MODEL_NAME,\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    ")\n",
    "pipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=lang, task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_list = [] \n",
    "file_list = [] \n",
    "time_list = [] \n",
    "directory = '/Path to .wav audio files'\n",
    "files = sorted(Path(directory).glob('*.wav'))\n",
    "for i, file in tqdm(enumerate(files)): \n",
    "  start = time.time() \n",
    "  transcription_list.append(pipe(str(file))['text'])\n",
    "  end = time.time() \n",
    "  file_list.append(str(file).split('/')[-1])\n",
    "  time_list.append(end-start)\n",
    "import pandas as pd \n",
    "df = pd.DataFrame({'File_no':file_list, \n",
    "                   'Transcription':transcription_list, \n",
    "                   'Execution_time':time_list}) \n",
    "df['Transcription'] = df['Transcription'].apply(clean_text) \n",
    "vals = df['Transcription'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate against Quran reference \n",
    "wer_list = [] \n",
    "mer_list = [] \n",
    "cer_list = [] \n",
    "wil_list = [] \n",
    "for i, j in zip(vals,vals_groundtruth): \n",
    "  wer_list.append(jiwer.wer(j,i)) \n",
    "  mer_list.append(jiwer.mer(j,i)) \n",
    "  cer_list.append(jiwer.cer(j,i)) \n",
    "  wil_list.append(jiwer.wil(j,i))\n",
    "df['WER'] = wer_list \n",
    "df['MER'] = mer_list \n",
    "df['CER'] = cer_list \n",
    "df['WIL'] = wil_list \n",
    "df.to_csv(\"whisperAR_eval.csv\",index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarteel - base\n",
    "MODEL_NAME = 'tarteel-ai/whisper-base-ar-quran'\n",
    "lang = 'ar'\n",
    "device = 0 if torch.cuda.is_available() else 'cpu' \n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=MODEL_NAME,\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    ")\n",
    "pipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=lang, task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_list = [] \n",
    "file_list = [] \n",
    "time_list = [] \n",
    "directory = '/Path to .wav audio files'\n",
    "files = sorted(Path(directory).glob('*.wav'))\n",
    "for i, file in tqdm(enumerate(files)): \n",
    "  start = time.time() \n",
    "  transcription_list.append(pipe(str(file))['text'])\n",
    "  end = time.time() \n",
    "  file_list.append(str(file).split('/')[-1])\n",
    "  time_list.append(end-start)\n",
    "import pandas as pd \n",
    "df = pd.DataFrame({'File_no':file_list, \n",
    "                   'Transcription':transcription_list, \n",
    "                   'Execution_time':time_list}) \n",
    "df['Transcription'] = df['Transcription'].apply(clean_text) \n",
    "vals = df['Transcription'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate against Quran reference \n",
    "wer_list = [] \n",
    "mer_list = [] \n",
    "cer_list = [] \n",
    "wil_list = [] \n",
    "for i, j in zip(vals,vals_groundtruth): \n",
    "  wer_list.append(jiwer.wer(j,i)) \n",
    "  mer_list.append(jiwer.mer(j,i)) \n",
    "  cer_list.append(jiwer.cer(j,i)) \n",
    "  wil_list.append(jiwer.wil(j,i))\n",
    "df['WER'] = wer_list \n",
    "df['MER'] = mer_list \n",
    "df['CER'] = cer_list \n",
    "df['WIL'] = wil_list \n",
    "df.to_csv(\"tarteel_base_eval.csv\",index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarteel - tiny\n",
    "MODEL_NAME = 'tarteel-ai/whisper-tiny-ar-quran'\n",
    "lang = 'ar'\n",
    "device = 0 if torch.cuda.is_available() else 'cpu' \n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=MODEL_NAME,\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    ")\n",
    "pipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=lang, task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_list = [] \n",
    "file_list = [] \n",
    "time_list = [] \n",
    "directory = '/Path to .wav audio files'\n",
    "files = sorted(Path(directory).glob('*.wav'))\n",
    "for i, file in tqdm(enumerate(files)): \n",
    "  start = time.time() \n",
    "  transcription_list.append(pipe(str(file))['text'])\n",
    "  end = time.time() \n",
    "  file_list.append(str(file).split('/')[-1])\n",
    "  time_list.append(end-start)\n",
    "import pandas as pd \n",
    "df = pd.DataFrame({'File_no':file_list, \n",
    "                   'Transcription':transcription_list, \n",
    "                   'Execution_time':time_list}) \n",
    "df['Transcription'] = df['Transcription'].apply(clean_text) \n",
    "vals = df['Transcription'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate against Quran reference \n",
    "wer_list = [] \n",
    "mer_list = [] \n",
    "cer_list = [] \n",
    "wil_list = [] \n",
    "for i, j in zip(vals,vals_groundtruth): \n",
    "  wer_list.append(jiwer.wer(j,i)) \n",
    "  mer_list.append(jiwer.mer(j,i)) \n",
    "  cer_list.append(jiwer.cer(j,i)) \n",
    "  wil_list.append(jiwer.wil(j,i))\n",
    "df['WER'] = wer_list \n",
    "df['MER'] = mer_list \n",
    "df['CER'] = cer_list \n",
    "df['WIL'] = wil_list \n",
    "df.to_csv(\"tarteel_tiny_eval.csv\",index=False) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
